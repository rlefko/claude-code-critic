"""
Integration tests for the complete indexing workflow.

Tests the interaction between parser, embedder, and storage components
during the full indexing process.
"""

import json
import os
from pathlib import Path
from unittest.mock import Mock, patch

import numpy as np
import pytest

from claude_indexer.config import IndexerConfig, load_config
from claude_indexer.indexer import CoreIndexer
from tests.conftest import get_unique_collection_name


def no_errors_in_logs(stderr_output: str, stdout_output: str) -> bool:
    """Helper function to check if there are no errors in the log output."""
    # Check stderr for actual errors
    if stderr_output.strip():
        # Allow some non-error output in stderr (like debug messages and warnings)
        error_indicators = ["error:", "exception:", "traceback", "âŒ"]
        # More specific error patterns, excluding common warnings
        strict_errors = [
            "error: ",
            "exception: ",
            "traceback:",
            "âŒ",
            "critical:",
            "fatal:",
        ]

        # Skip benign warnings like "Failed to get global entities" on new collections
        benign_patterns = [
            "failed to get global entities",
            "collection `",
            "doesn't exist",
            "warning",
            "info",
            "debug",
        ]

        # Check if any line contains errors but isn't benign
        for line in stderr_output.lower().split("\n"):
            line = line.strip()
            if any(error in line for error in strict_errors):
                # Skip if it's a benign warning
                if not any(pattern in line for pattern in benign_patterns):
                    return False

    # Check stdout for error indicators
    error_indicators = ["âŒ", "error:", "exception:", "traceback"]
    return not any(indicator in stdout_output.lower() for indicator in error_indicators)


def validate_state_file_structure(state_file_path: str, expected_files: set) -> dict:
    """Validate state file JSON structure and file contents."""
    import json

    assert Path(
        state_file_path
    ).exists(), f"State file should exist at {state_file_path}"

    with open(state_file_path) as f:
        state_data = json.load(f)

    # Validate JSON structure
    assert isinstance(state_data, dict), "State file should contain a JSON object"

    # Check expected files are present
    state_files = set(state_data.keys())
    for expected_file in expected_files:
        assert (
            expected_file in state_files
        ), f"Expected file {expected_file} not in state: {state_files}"

    # Validate each file's metadata structure
    for file_path, file_state in state_data.items():
        assert isinstance(
            file_state, dict
        ), f"File state for {file_path} should be an object"

        # Skip validation for special metadata entries like _statistics
        if file_path.startswith("_"):
            continue

        assert "hash" in file_state, f"Hash missing for {file_path}"
        assert "size" in file_state, f"Size missing for {file_path}"
        assert "mtime" in file_state, f"Mtime missing for {file_path}"

        # Validate hash format (SHA256 should be 64 characters)
        assert (
            len(file_state["hash"]) == 64
        ), f"Invalid hash length for {file_path}: {file_state['hash']}"
        assert isinstance(
            file_state["size"], int
        ), f"Size should be integer for {file_path}"
        assert isinstance(
            file_state["mtime"], (int, float)
        ), f"Mtime should be number for {file_path}"

    return state_data


def extract_deletion_info_from_cli_output(output: str) -> dict:
    """Extract deletion information from verbose CLI output."""
    lines = output.split("\n")
    deleted_files = []
    handled_deletions = 0
    orphan_cleanups = 0

    for line in lines:
        line_lower = line.lower()

        # Look for deletion patterns in verbose output
        if "ðŸ—‘ï¸ handling deleted file" in line_lower or "deleted file:" in line_lower:
            handled_deletions += 1
            # Try to extract filename
            if ".py" in line:
                # Extract filename from various patterns
                parts = line.split()
                for part in parts:
                    if part.endswith(".py") and not part.startswith("ðŸ—‘ï¸"):
                        deleted_files.append(part)
                        break

        # Look for orphan relation cleanup
        if "ðŸ—‘ï¸ deleted" in line_lower and "orphaned relation" in line_lower:
            orphan_cleanups += 1

    return {
        "deleted_files": deleted_files,
        "handled_deletions": handled_deletions,
        "orphan_cleanups": orphan_cleanups,
    }


def extract_processed_files_from_cli_output(output: str) -> list:
    """Extract processed filenames from verbose CLI output."""
    lines = output.split("\n")
    processed_files = []

    for line in lines:
        line_lower = line.lower()

        # Look for file processing patterns
        if ("processing" in line_lower or "indexing" in line_lower) and ".py" in line:
            # Extract filename from various possible patterns
            if "file:" in line_lower:
                parts = line.split("file:")
                if len(parts) > 1:
                    filename = parts[1].strip().split()[0]
                    if filename.endswith(".py"):
                        processed_files.append(filename)
            elif "batch" in line_lower:
                # Look for batch processing indicators
                continue
            else:
                # Try to find .py files in the line
                parts = line.split()
                for part in parts:
                    if part.endswith(".py") and not part.startswith("ðŸ”„"):
                        processed_files.append(part)

    return processed_files


@pytest.mark.integration
class TestACustomFlow:
    """Test complete indexing workflows."""

    def test_full_index_flow_with_real_files(
        self, temp_repo, dummy_embedder, qdrant_store
    ):
        """Test complete indexing flow with real Python files."""
        # Use unique collection name to avoid collisions
        collection_name = get_unique_collection_name("test_integration")

        # Load real API keys from settings.txt instead of using hardcoded test keys
        base_config = load_config()
        config = IndexerConfig(
            openai_api_key=base_config.openai_api_key,
            qdrant_api_key=base_config.qdrant_api_key,
            qdrant_url=base_config.qdrant_url,
        )

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=temp_repo,
        )

        # Index the temporary repository
        result = indexer.index_project(collection_name)

        # Verify indexing succeeded
        assert result.success is True
        assert result.entities_created >= 3  # At least foo.py, bar.py, helpers.py
        assert result.relations_created >= 1  # At least one import relation

        # Verify vectors were stored
        count = qdrant_store.count(collection_name)
        assert count >= 3, f"Expected at least 3 vectors, got {count}"

        # Verify we can find content using payload query (deterministic)
        from tests.conftest import get_entities_by_file_path

        foo_entities = get_entities_by_file_path(
            qdrant_store, collection_name, "foo.py", verbose=True
        )
        assert len(foo_entities) > 0, "Should find entities from foo.py"

    def test_incremental_indexing_flow(self, temp_repo, dummy_embedder, qdrant_store):
        """Test incremental indexing with file changes."""
        # Use unique collection name to avoid collisions
        collection_name = get_unique_collection_name("test_incremental")

        # Load real API keys from settings.txt instead of using hardcoded test keys
        base_config = load_config()
        config = IndexerConfig(
            openai_api_key=base_config.openai_api_key,
            qdrant_api_key=base_config.qdrant_api_key,
            qdrant_url=base_config.qdrant_url,
        )

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=temp_repo,
        )

        # Initial index
        indexer.index_project(collection_name)
        initial_count = qdrant_store.count(collection_name)

        # Modify a file
        modified_file = temp_repo / "foo.py"
        original_content = modified_file.read_text()
        modified_content = (
            original_content
            + '\n\ndef subtract(x, y):\n    """Subtract two numbers."""\n    return x - y\n'
        )
        modified_file.write_text(modified_content)

        # Second index (should auto-detect incremental mode)
        result2 = indexer.index_project(collection_name)
        final_count = qdrant_store.count(collection_name)

        # Verify incremental indexing worked
        assert result2.success is True
        assert final_count >= initial_count  # Should have same or more vectors

        # Use scroll with payload filtering instead of semantic search
        # DummyEmbedder doesn't have semantic similarity, so we verify via direct DB query
        from tests.conftest import (
            get_file_path_from_payload,
            wait_for_eventual_consistency,
        )

        def find_subtract_entity():
            """Search for subtract entity using scroll (bypasses embedding similarity)."""
            scroll_result = qdrant_store.client.scroll(
                collection_name=collection_name, limit=200, with_payload=True
            )
            entities = scroll_result[0] if scroll_result else []
            return [
                entity
                for entity in entities
                if "subtract" in entity.payload.get("entity_name", "").lower()
                or "subtract" in entity.payload.get("name", "").lower()
                or "subtract" in entity.payload.get("content", "").lower()
            ]

        # Wait for eventual consistency
        wait_for_eventual_consistency(
            find_subtract_entity, expected_count=1, timeout=30.0, verbose=True
        )

        # Final verification - scroll through collection to find subtract
        scroll_result = qdrant_store.client.scroll(
            collection_name=collection_name, limit=200, with_payload=True
        )
        all_entities = scroll_result[0] if scroll_result else []

        print(f"Total entities in collection: {len(all_entities)}")
        for entity in all_entities[:10]:  # Show first 10
            name = entity.payload.get("name", "N/A")
            file_path = get_file_path_from_payload(entity.payload) or "N/A"
            print(f"  - {name} (from {file_path})")

        # Look for subtract function in all entities
        subtract_found = any(
            "subtract" in entity.payload.get("entity_name", "").lower()
            or "subtract" in entity.payload.get("name", "").lower()
            or "subtract" in entity.payload.get("content", "").lower()
            or "subtract" in str(entity.payload).lower()
            for entity in all_entities
        )

        # Enhanced debugging if test fails
        if not subtract_found:
            print("DEBUG: All entity names in collection:")
            for entity in all_entities:
                name = entity.payload.get(
                    "name", entity.payload.get("entity_name", "N/A")
                )
                print(f"  - {name}")

        assert (
            subtract_found
        ), f"subtract function not found in {len(all_entities)} entities"

    def test_error_handling_in_flow(self, temp_repo, dummy_embedder, qdrant_store):
        """Test error handling during indexing flow."""
        collection_name = get_unique_collection_name("test_errors")
        config = IndexerConfig(
            collection_name=collection_name,
            embedder_type="dummy",
            storage_type="qdrant",
        )

        # Create a file with syntax errors
        bad_file = temp_repo / "bad_syntax.py"
        bad_file.write_text("def broken(\n    return 'invalid syntax'")

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=temp_repo,
        )

        # Indexing should still succeed for valid files
        result = indexer.index_project(collection_name)

        # Should be successful overall despite individual file errors
        assert result.success is True
        assert result.entities_created >= 2  # Valid files still processed
        # Note: Errors may be tracked or silently skipped depending on implementation
        # The key assertion is that the indexer doesn't crash and processes valid files

    def test_empty_project_indexing(self, empty_repo, dummy_embedder, qdrant_store):
        """Test indexing an empty project."""
        collection_name = get_unique_collection_name("test_empty")
        config = IndexerConfig(
            collection_name=collection_name,
            embedder_type="dummy",
            storage_type="qdrant",
        )

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=empty_repo,
        )

        result = indexer.index_project(collection_name)

        # Should succeed with no entities
        assert result.success is True
        assert result.entities_created == 0
        assert result.relations_created == 0
        assert qdrant_store.count(collection_name) == 0

    def test_large_file_batching(self, tmp_path, dummy_embedder, qdrant_store):
        """Test indexing with many files to verify batching."""
        collection_name = get_unique_collection_name("test_batching")
        config = IndexerConfig(
            collection_name=collection_name,
            embedder_type="dummy",
            storage_type="qdrant",
        )

        # Create many small Python files
        for i in range(20):
            py_file = tmp_path / f"module_{i}.py"
            py_file.write_text(
                f'''"""Module {i}."""

def function_{i}():
    """Function number {i}."""
    return {i}

CLASS_{i} = "constant_{i}"
'''
            )

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=tmp_path,
        )

        result = indexer.index_project(collection_name)

        # Should successfully process all files
        assert result.success is True
        assert result.entities_created >= 40  # At least 2 entities per file
        assert qdrant_store.count(collection_name) >= 40

    def test_duplicate_entity_handling(self, tmp_path, dummy_embedder, qdrant_store):
        """Test handling of duplicate entities across files."""
        collection_name = get_unique_collection_name("test_duplicates")
        config = IndexerConfig(
            collection_name=collection_name,
            embedder_type="dummy",
            storage_type="qdrant",
        )

        # Create files with same function names
        file1 = tmp_path / "module1.py"
        file1.write_text(
            '''
def common_function():
    """First implementation."""
    return 1
'''
        )

        file2 = tmp_path / "module2.py"
        file2.write_text(
            '''
def common_function():
    """Second implementation."""
    return 2
'''
        )

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=tmp_path,
        )

        result = indexer.index_project(collection_name)

        # Should handle duplicates gracefully
        assert result.success is True

        # Verify both files are indexed using payload queries (deterministic)
        from tests.conftest import get_entities_by_file_path

        module1_entities = get_entities_by_file_path(
            qdrant_store, collection_name, "module1.py", verbose=True
        )
        module2_entities = get_entities_by_file_path(
            qdrant_store, collection_name, "module2.py", verbose=True
        )
        assert len(module1_entities) > 0, "module1.py should be indexed"
        assert len(module2_entities) > 0, "module2.py should be indexed"


@pytest.mark.integration
class TestIndexerConfiguration:
    """Test indexer configuration and initialization."""

    def test_indexer_with_different_embedders(self, temp_repo, qdrant_store):
        """Test indexer with different embedder configurations."""
        collection_name = get_unique_collection_name("test_embedders")
        # Test with dummy embedder
        config = IndexerConfig(
            collection_name=collection_name,
            embedder_type="dummy",
            storage_type="qdrant",
        )

        with patch(
            "claude_indexer.embeddings.registry.create_embedder_from_config"
        ) as mock_create:
            from claude_indexer.embeddings.base import EmbeddingResult

            mock_embedder = Mock()
            mock_embedder.dimension = 1536  # Required for collection creation

            # Mock embed_text method to return EmbeddingResult
            def mock_embed_text(text):
                return EmbeddingResult(
                    text=text,
                    embedding=np.random.rand(1536).astype(np.float32).tolist(),
                    model="mock-model",
                    token_count=len(text.split()),
                    processing_time=0.001,
                    cost_estimate=0.0001,
                )

            # Mock embed_batch method to return list of EmbeddingResult
            def mock_embed_batch(texts, **kwargs):
                return [mock_embed_text(text) for text in texts]

            mock_embedder.embed_text.side_effect = mock_embed_text
            mock_embedder.embed_batch.side_effect = mock_embed_batch

            # Mock get_usage_stats to return proper dict structure
            mock_embedder.get_usage_stats.return_value = {
                "total_requests": 0,
                "total_tokens": 0,
                "total_cost": 0.0,
            }

            mock_create.return_value = mock_embedder

            indexer = CoreIndexer(
                config=config,
                embedder=mock_embedder,
                vector_store=qdrant_store,
                project_path=temp_repo,
            )

            result = indexer.index_project(collection_name)
            assert result.success is True

            # Verify embedder was used
            assert mock_embedder.embed_text.called or mock_embedder.embed_batch.called

    def test_indexer_with_custom_filters(self, temp_repo, dummy_embedder, qdrant_store):
        """Test indexer with custom file filters."""
        collection_name = get_unique_collection_name("test_filters")
        config = IndexerConfig(
            collection_name=collection_name,
            embedder_type="dummy",
            storage_type="qdrant",
            include_patterns=["*.py"],
            exclude_patterns=["test_*"],
        )

        # Add test files that should be excluded
        test_dir = temp_repo / "tests"
        test_dir.mkdir(exist_ok=True)
        (test_dir / "test_example.py").write_text("def test_something(): pass")

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=temp_repo,
        )

        result = indexer.index_project(collection_name)

        # Should exclude test files
        assert result.success is True

        # Verify test files were not indexed using payload query (deterministic)
        from tests.conftest import get_entities_by_file_path

        test_entities = get_entities_by_file_path(
            qdrant_store, collection_name, "test_", verbose=True
        )
        test_files_found = len(test_entities) > 0
        assert not test_files_found, (
            f"Test files should be excluded by filter. "
            f"Found {len(test_entities)} entities from test files."
        )


@pytest.mark.integration
class TestIndexerPerformance:
    """Test indexer performance characteristics."""

    def test_indexing_performance_tracking(
        self, temp_repo, dummy_embedder, qdrant_store
    ):
        """Test that indexing tracks performance metrics."""
        collection_name = get_unique_collection_name("test_performance")
        config = IndexerConfig(
            collection_name=collection_name,
            embedder_type="dummy",
            storage_type="qdrant",
        )

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=temp_repo,
        )

        result = indexer.index_project(collection_name)

        # Should track timing information
        assert result.success is True
        assert hasattr(result, "duration")
        assert result.duration >= 0

        # Should have file-level statistics
        assert result.files_processed >= 3
        assert result.entities_created >= 3

    def test_memory_efficient_processing(self, tmp_path, dummy_embedder, qdrant_store):
        """Test that large projects don't consume excessive memory."""
        collection_name = get_unique_collection_name("test_memory")
        config = IndexerConfig(
            collection_name=collection_name,
            embedder_type="dummy",
            storage_type="qdrant",
        )

        # Create larger files to test memory usage
        for i in range(5):
            large_file = tmp_path / f"large_{i}.py"
            content = f'"""Large module {i}."""\n\n'

            # Add many functions to create a larger file
            for j in range(50):
                content += f'''
def function_{i}_{j}(param_{j}):
    """Function {i}-{j} with parameter."""
    result = "processing_{i}_{j}"
    return result
'''
            large_file.write_text(content)

        indexer = CoreIndexer(
            config=config,
            embedder=dummy_embedder,
            vector_store=qdrant_store,
            project_path=tmp_path,
        )

        # Should process without memory issues
        result = indexer.index_project(collection_name)

        assert result.success is True
        assert result.entities_created >= 250  # 5 files * 50 functions each
        assert qdrant_store.count(collection_name) >= 250


@pytest.mark.integration
@pytest.mark.skipif(
    not os.environ.get("OPENAI_API_KEY"),
    reason="Requires OPENAI_API_KEY environment variable for CLI tests",
)
class TestACustomIncrementalBehavior:
    """Custom tests for precise incremental indexing behavior verification.

    Note: These tests use CLI with real embedders and require OPENAI_API_KEY.
    They are skipped in CI environments without API keys.
    """

    def test_custom_single_new_file_processing(
        self, temp_repo, dummy_embedder, qdrant_store
    ):
        """Test that exactly 1 new file is processed in incremental mode using CLI."""
        import subprocess

        # Create settings file for CLI with real API keys
        base_config = load_config()
        settings_file = temp_repo / "settings.txt"
        settings_file.write_text(
            f"""
openai_api_key={base_config.openai_api_key}
qdrant_api_key={base_config.qdrant_api_key}
qdrant_url={base_config.qdrant_url}
"""
        )

        # Run initial CLI indexing - should be full mode (auto-detected)
        result1 = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "index",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_new_file",
                "--verbose",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        initial_output = result1.stdout
        initial_errors = result1.stderr

        assert result1.returncode == 0, f"Initial indexing failed: {initial_errors}"
        initial_count = qdrant_store.count("test_custom_new_file")

        # CONSOLE LOG CHECKS - Initial indexing should show CLI output
        assert (
            "Mode: Full" in initial_output
            or "files to process" in initial_output.lower()
        ), f"Initial CLI should show mode or processing activity. Got: {initial_output}"
        assert no_errors_in_logs(
            initial_errors, initial_output
        ), f"Should have no errors in initial indexing. Errors: {initial_errors}"

        # Find state file location (CLI uses project-local state directory)
        state_dir = temp_repo / ".claude-indexer"
        state_file = state_dir / "test_custom_new_file.json"

        # Verify state file was created after initial index
        assert state_file.exists(), f"State file should exist at {state_file}"

        # Load initial state file content
        with open(state_file) as f:
            initial_state = json.load(f)
        initial_state_file_count = len(initial_state)

        # Add exactly 1 new file
        new_file = temp_repo / "new_module.py"
        new_file.write_text(
            '''
def new_function():
    """A new function to test incremental indexing."""
    return "new functionality"

NEW_CONSTANT = "test_value"
'''
        )

        # Run incremental CLI indexing - should auto-detect incremental mode
        result2 = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "index",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_new_file",
                "--verbose",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        incremental_output = result2.stdout
        incremental_errors = result2.stderr

        assert (
            result2.returncode == 0
        ), f"Incremental indexing failed: {incremental_errors}"
        final_count = qdrant_store.count("test_custom_new_file")

        # CONSOLE LOG CHECKS - Incremental indexing should show CLI mode output
        assert (
            "Mode: Incremental" in incremental_output
            or "1 files to process" in incremental_output
        ), f"Should show incremental mode or file count. Got: {incremental_output}"
        # Check that incremental mode processed the new file
        assert any(
            phrase in incremental_output
            for phrase in [
                "Files processed: 1",
                "Total Vectored Files:",
                "files to process",
            ]
        ), f"Should show file processing activity. Got: {incremental_output}"
        assert no_errors_in_logs(
            incremental_errors, incremental_output
        ), f"Should have no errors in incremental indexing. Errors: {incremental_errors}"

        # ENHANCED FILENAME VALIDATION - Parse CLI verbose output and validate state
        extract_processed_files_from_cli_output(incremental_output)

        # ENHANCED STATE FILE VALIDATION
        # Filter out metadata keys (those starting with _) from expected files
        initial_files = {k for k in initial_state if not k.startswith("_")}
        expected_files_in_final_state = initial_files | {"new_module.py"}
        validate_state_file_structure(str(state_file), expected_files_in_final_state)

        # Verify vector count increased (new entities added)
        assert (
            final_count > initial_count
        ), f"Vector count should increase from {initial_count} to {final_count}"

        # Verify state file was updated with exactly 1 additional file
        with open(state_file) as f:
            final_state = json.load(f)
        final_state_file_count = len(final_state)

        assert (
            final_state_file_count == initial_state_file_count + 1
        ), f"State file should have {initial_state_file_count + 1} files, got {final_state_file_count}"
        assert "new_module.py" in final_state, "new_module.py should be in state file"

        # Verify state file contains correct metadata for new file
        new_file_state = final_state["new_module.py"]
        assert "hash" in new_file_state, "State should contain hash for new file"
        assert "size" in new_file_state, "State should contain size for new file"
        assert "mtime" in new_file_state, "State should contain mtime for new file"

        # Verify we can find the new function using CLI search
        search_result = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "search",
                "new_function",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_new_file",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        assert search_result.returncode == 0, f"Search failed: {search_result.stderr}"
        assert (
            "new_function" in search_result.stdout
        ), "Should find new_function in search results"

    def test_custom_single_file_deletion(self, temp_repo, dummy_embedder, qdrant_store):
        """Test that exactly 1 deleted file is processed in incremental mode using CLI."""
        import subprocess

        # Create settings file for CLI with real API keys
        base_config = load_config()
        settings_file = temp_repo / "settings.txt"
        settings_file.write_text(
            f"""
openai_api_key={base_config.openai_api_key}
qdrant_api_key={base_config.qdrant_api_key}
qdrant_url={base_config.qdrant_url}
"""
        )

        # Create an additional file to delete later
        deletable_file = temp_repo / "deletable.py"
        deletable_file.write_text(
            '''
def deletable_function():
    """This function will be deleted."""
    return "will be deleted"

DELETABLE_CONSTANT = "to_be_removed"
'''
        )

        # Run initial CLI indexing - should be full mode (auto-detected)
        result1 = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "index",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_deletion",
                "--verbose",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        initial_output = result1.stdout
        initial_errors = result1.stderr

        assert result1.returncode == 0, f"Initial indexing failed: {initial_errors}"
        initial_count = qdrant_store.count("test_custom_deletion")

        # CONSOLE LOG CHECKS - Initial indexing should show CLI output
        assert (
            "Mode: Full" in initial_output
            or "files to process" in initial_output.lower()
        ), f"Initial CLI should show mode or processing activity. Got: {initial_output}"
        assert no_errors_in_logs(
            initial_errors, initial_output
        ), f"Should have no errors in initial indexing. Errors: {initial_errors}"

        # Find state file location (CLI uses project-local state directory)
        state_dir = temp_repo / ".claude-indexer"
        state_file = state_dir / "test_custom_deletion.json"

        # Verify state file was created and contains deletable file
        assert state_file.exists(), f"State file should exist at {state_file}"

        with open(state_file) as f:
            initial_state = json.load(f)
        # Count only actual files, not metadata keys
        initial_state_file_count = sum(
            1 for k in initial_state if not k.startswith("_")
        )
        assert (
            "deletable.py" in initial_state
        ), "deletable.py should be in initial state"

        # Delete exactly 1 file
        deletable_file.unlink()

        # Run incremental CLI indexing - should auto-detect incremental mode and handle deletion
        result2 = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "index",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_deletion",
                "--verbose",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        deletion_output = result2.stdout
        deletion_errors = result2.stderr

        assert result2.returncode == 0, f"Deletion indexing failed: {deletion_errors}"
        final_count = qdrant_store.count("test_custom_deletion")

        # CONSOLE LOG CHECKS - Deletion processing should show CLI mode output
        assert (
            "Mode: Incremental" in deletion_output
        ), f"Should show incremental mode. Got: {deletion_output}"
        # Check that deletion mode shows appropriate output
        assert any(
            phrase in deletion_output
            for phrase in [
                "Files processed: 0",
                "No files to process",
                "deleted files",
                "Handled",
            ]
        ), f"Should show deletion or no processing activity. Got: {deletion_output}"
        assert no_errors_in_logs(
            deletion_errors, deletion_output
        ), f"Should have no errors in deletion processing. Errors: {deletion_errors}"

        # ENHANCED DELETION VALIDATION - Parse CLI verbose output for deletion information
        extract_deletion_info_from_cli_output(deletion_output)

        # ENHANCED STATE FILE VALIDATION FOR DELETION
        # Filter out metadata keys (those starting with _) from expected files
        initial_files = {k for k in initial_state if not k.startswith("_")}
        expected_files_after_deletion = initial_files - {"deletable.py"}
        validate_state_file_structure(str(state_file), expected_files_after_deletion)

        # Verify vector count decreased (entities removed)
        assert (
            final_count < initial_count
        ), f"Vector count should decrease from {initial_count} to {final_count}"

        # Verify state file was updated with file removed
        with open(state_file) as f:
            final_state = json.load(f)
        # Count only actual files, not metadata keys
        final_state_file_count = sum(1 for k in final_state if not k.startswith("_"))

        assert (
            final_state_file_count == initial_state_file_count - 1
        ), f"State file should have {initial_state_file_count - 1} files, got {final_state_file_count}"
        assert (
            "deletable.py" not in final_state
        ), "deletable.py should not be in final state"

        # Verify we can no longer find the deleted function using CLI search
        search_result = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "search",
                "deletable_function",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_deletion",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        assert search_result.returncode == 0, f"Search failed: {search_result.stderr}"
        # The search output will show "Found X results for: deletable_function"
        # but should not show "deletable_function" as an actual result entity
        # Check that no line starts with a number and contains "deletable_function" as the entity name
        lines = search_result.stdout.split("\n")
        for line in lines:
            # Result lines look like: "1. entity_name (score: 0.123)"
            if line.strip() and line[0].isdigit() and ". " in line:
                entity_name = line.split(". ", 1)[1].split(" (score:")[0]
                assert (
                    entity_name != "deletable_function"
                ), f"Found deleted function in results: {line}"

        # Verify remaining files from temp_repo are still indexed
        remaining_files = set(final_state.keys())
        expected_remaining = {
            "foo.py",
            "bar.py",
            "utils/helpers.py",
            "utils/__init__.py",
        }
        assert expected_remaining.issubset(
            remaining_files
        ), f"Expected remaining files {expected_remaining}, got {remaining_files}"

    def test_custom_three_new_files_processing(
        self, temp_repo, dummy_embedder, qdrant_store
    ):
        """Test that exactly 3 new files are processed in incremental mode using CLI."""
        import subprocess

        # Create settings file for CLI with real API keys
        base_config = load_config()
        settings_file = temp_repo / "settings.txt"
        settings_file.write_text(
            f"""
openai_api_key={base_config.openai_api_key}
qdrant_api_key={base_config.qdrant_api_key}
qdrant_url={base_config.qdrant_url}
"""
        )

        # Run initial CLI indexing - should be full mode (auto-detected)
        result1 = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "index",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_three_new",
                "--verbose",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        initial_output = result1.stdout
        initial_errors = result1.stderr

        assert result1.returncode == 0, f"Initial indexing failed: {initial_errors}"
        initial_count = qdrant_store.count("test_custom_three_new")

        # CONSOLE LOG CHECKS - Initial indexing should show CLI output
        assert (
            "Mode: Full" in initial_output
            or "files to process" in initial_output.lower()
        ), f"Initial CLI should show mode or processing activity. Got: {initial_output}"
        assert no_errors_in_logs(
            initial_errors, initial_output
        ), f"Should have no errors in initial indexing. Errors: {initial_errors}"

        # Find state file location (CLI uses project-local state directory)
        state_dir = temp_repo / ".claude-indexer"
        state_file = state_dir / "test_custom_three_new.json"

        # Verify state file was created after initial index
        assert state_file.exists(), f"State file should exist at {state_file}"

        # Load initial state file content
        with open(state_file) as f:
            initial_state = json.load(f)
        initial_state_file_count = len(initial_state)

        # Add exactly 3 new files
        new_files = []
        for i in range(1, 4):
            new_file = temp_repo / f"new_module_{i}.py"
            new_file.write_text(
                f'''
def new_function_{i}():
    """A new function {i} to test incremental indexing."""
    return "new functionality {i}"

NEW_CONSTANT_{i} = "test_value_{i}"

class NewClass_{i}:
    """A new class {i} for testing."""
    def method_{i}(self):
        return {i}
'''
            )
            new_files.append(new_file)

        # Run incremental CLI indexing - should auto-detect incremental mode and process exactly 3 files
        result2 = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "index",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_three_new",
                "--verbose",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        incremental_output = result2.stdout
        incremental_errors = result2.stderr

        assert (
            result2.returncode == 0
        ), f"Incremental indexing failed: {incremental_errors}"
        final_count = qdrant_store.count("test_custom_three_new")

        # CONSOLE LOG CHECKS - Incremental indexing should show CLI mode output
        assert (
            "Mode: Incremental" in incremental_output
        ), f"Should show incremental mode. Got: {incremental_output}"
        # Check that incremental mode processed multiple files
        assert any(
            phrase in incremental_output
            for phrase in [
                "Files processed: 3",
                "Total Vectored Files:",
                "files to process",
            ]
        ), f"Should show file processing activity. Got: {incremental_output}"
        assert no_errors_in_logs(
            incremental_errors, incremental_output
        ), f"Should have no errors in incremental indexing. Errors: {incremental_errors}"

        # ENHANCED FILENAME VALIDATION - Parse CLI verbose output and validate state
        extract_processed_files_from_cli_output(incremental_output)

        # ENHANCED STATE FILE VALIDATION
        new_file_names = {f"new_module_{i}.py" for i in range(1, 4)}
        # Filter out metadata keys (those starting with _) from expected files
        initial_files = {k for k in initial_state if not k.startswith("_")}
        expected_files_in_final_state = initial_files | new_file_names
        validate_state_file_structure(str(state_file), expected_files_in_final_state)

        # Verify vector count increased significantly (new entities added)
        assert (
            final_count > initial_count
        ), f"Vector count should increase from {initial_count} to {final_count}"

        # Verify state file was updated with exactly 3 additional files
        with open(state_file) as f:
            final_state = json.load(f)
        final_state_file_count = len(final_state)

        assert (
            final_state_file_count == initial_state_file_count + 3
        ), f"State file should have {initial_state_file_count + 3} files, got {final_state_file_count}"

        # Verify all 3 new files are in state file
        for i in range(1, 4):
            assert (
                f"new_module_{i}.py" in final_state
            ), f"new_module_{i}.py should be in state file"

            # Verify state file contains correct metadata for each new file
            new_file_state = final_state[f"new_module_{i}.py"]
            assert (
                "hash" in new_file_state
            ), f"State should contain hash for new_module_{i}.py"
            assert (
                "size" in new_file_state
            ), f"State should contain size for new_module_{i}.py"
            assert (
                "mtime" in new_file_state
            ), f"State should contain mtime for new_module_{i}.py"

        # Verify we can find the new functions using CLI search
        for i in range(1, 4):
            search_result = subprocess.run(
                [
                    "python",
                    "-m",
                    "claude_indexer",
                    "search",
                    f"new_function_{i}",
                    "--project",
                    str(temp_repo),
                    "--collection",
                    "test_custom_three_new",
                ],
                capture_output=True,
                text=True,
                cwd=temp_repo,
                timeout=120,
            )

            assert (
                search_result.returncode == 0
            ), f"Search failed for new_function_{i}: {search_result.stderr}"
            assert (
                f"new_function_{i}" in search_result.stdout
            ), f"Should find new_function_{i} in search results"

    def test_custom_three_files_deletion(self, temp_repo, dummy_embedder, qdrant_store):
        """Test that exactly 3 deleted files are processed in incremental mode using CLI."""
        import subprocess

        # Create settings file for CLI with real API keys
        base_config = load_config()
        settings_file = temp_repo / "settings.txt"
        settings_file.write_text(
            f"""
openai_api_key={base_config.openai_api_key}
qdrant_api_key={base_config.qdrant_api_key}
qdrant_url={base_config.qdrant_url}
"""
        )

        # Create 3 additional files to delete later
        deletable_files = []
        for i in range(1, 4):
            deletable_file = temp_repo / f"deletable_{i}.py"
            deletable_file.write_text(
                f'''
def deletable_function_{i}():
    """This function {i} will be deleted."""
    return "will be deleted {i}"

DELETABLE_CONSTANT_{i} = "to_be_removed_{i}"

class DeletableClass_{i}:
    """A deletable class {i}."""
    def deletable_method_{i}(self):
        return "deletable_{i}"
'''
            )
            deletable_files.append(deletable_file)

        # Run initial CLI indexing - should be full mode (auto-detected)
        result1 = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "index",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_three_deletion",
                "--verbose",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        initial_output = result1.stdout
        initial_errors = result1.stderr

        assert result1.returncode == 0, f"Initial indexing failed: {initial_errors}"
        initial_count = qdrant_store.count("test_custom_three_deletion")

        # CONSOLE LOG CHECKS - Initial indexing should show CLI output
        assert (
            "Mode: Full" in initial_output
            or "files to process" in initial_output.lower()
        ), f"Initial CLI should show mode or processing activity. Got: {initial_output}"
        assert no_errors_in_logs(
            initial_errors, initial_output
        ), f"Should have no errors in initial indexing. Errors: {initial_errors}"

        # Find state file location (CLI uses project-local state directory)
        state_dir = temp_repo / ".claude-indexer"
        state_file = state_dir / "test_custom_three_deletion.json"

        # Verify state file was created and contains all deletable files
        assert state_file.exists(), f"State file should exist at {state_file}"

        with open(state_file) as f:
            initial_state = json.load(f)
        initial_state_file_count = len(initial_state)

        # Verify all 3 deletable files are in initial state
        for i in range(1, 4):
            assert (
                f"deletable_{i}.py" in initial_state
            ), f"deletable_{i}.py should be in initial state"

        # Delete exactly 3 files
        for deletable_file in deletable_files:
            deletable_file.unlink()

        # Run incremental CLI indexing - should auto-detect incremental mode and process all 3 deletions
        result2 = subprocess.run(
            [
                "python",
                "-m",
                "claude_indexer",
                "index",
                "--project",
                str(temp_repo),
                "--collection",
                "test_custom_three_deletion",
                "--verbose",
            ],
            capture_output=True,
            text=True,
            cwd=temp_repo,
            timeout=120,
        )

        deletion_output = result2.stdout
        deletion_errors = result2.stderr

        assert result2.returncode == 0, f"Deletion indexing failed: {deletion_errors}"
        final_count = qdrant_store.count("test_custom_three_deletion")

        # CONSOLE LOG CHECKS - Deletion processing should show CLI mode output
        assert (
            "Mode: Incremental" in deletion_output
        ), f"Should show incremental mode. Got: {deletion_output}"
        # Check that deletion mode shows appropriate output
        assert any(
            phrase in deletion_output
            for phrase in [
                "Files processed: 0",
                "No files to process",
                "deleted files",
                "Handled",
            ]
        ), f"Should show deletion or no processing activity. Got: {deletion_output}"
        assert no_errors_in_logs(
            deletion_errors, deletion_output
        ), f"Should have no errors in deletion processing. Errors: {deletion_errors}"

        # ENHANCED DELETION VALIDATION - Parse CLI verbose output for deletion information
        extract_deletion_info_from_cli_output(deletion_output)

        # ENHANCED STATE FILE VALIDATION FOR THREE-FILE DELETION
        deleted_file_names = {f"deletable_{i}.py" for i in range(1, 4)}
        # Filter out metadata keys (those starting with _) from expected files
        initial_files = {k for k in initial_state if not k.startswith("_")}
        expected_files_after_deletion = initial_files - deleted_file_names
        validate_state_file_structure(str(state_file), expected_files_after_deletion)

        # Verify vector count decreased significantly (entities removed)
        assert (
            final_count < initial_count
        ), f"Vector count should decrease from {initial_count} to {final_count}"

        # The decrease should be substantial (3 files worth of entities)
        count_decrease = initial_count - final_count
        assert (
            count_decrease >= 6
        ), f"Expected significant decrease (>=6), got {count_decrease}"

        # Verify state file was updated with all 3 files removed
        with open(state_file) as f:
            final_state = json.load(f)
        final_state_file_count = len(final_state)

        assert (
            final_state_file_count == initial_state_file_count - 3
        ), f"State file should have {initial_state_file_count - 3} files, got {final_state_file_count}"

        # Verify all 3 deletable files are no longer in state file
        for i in range(1, 4):
            assert (
                f"deletable_{i}.py" not in final_state
            ), f"deletable_{i}.py should not be in final state"

        # Verify we can no longer find any of the deleted functions using CLI search
        for i in range(1, 4):
            search_result = subprocess.run(
                [
                    "python",
                    "-m",
                    "claude_indexer",
                    "search",
                    f"deletable_function_{i}",
                    "--project",
                    str(temp_repo),
                    "--collection",
                    "test_custom_three_deletion",
                ],
                capture_output=True,
                text=True,
                cwd=temp_repo,
                timeout=120,
            )

            assert (
                search_result.returncode == 0
            ), f"Search failed for deletable_function_{i}: {search_result.stderr}"
            # Check that no line starts with a number and contains "deletable_function_X" as the entity name
            lines = search_result.stdout.split("\n")
            for line in lines:
                # Result lines look like: "1. entity_name (score: 0.123)"
                if line.strip() and line[0].isdigit() and ". " in line:
                    entity_name = line.split(". ", 1)[1].split(" (score:")[0]
                    assert (
                        entity_name != f"deletable_function_{i}"
                    ), f"Found deleted function in results: {line}"

        # Verify remaining files from temp_repo are still indexed
        remaining_files = set(final_state.keys())
        expected_remaining = {
            "foo.py",
            "bar.py",
            "utils/helpers.py",
            "utils/__init__.py",
        }
        assert expected_remaining.issubset(
            remaining_files
        ), f"Expected remaining files {expected_remaining}, got {remaining_files}"
