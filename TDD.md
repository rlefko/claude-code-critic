Enhanced Claude Code Memory and Quality Guard System

Technical Design Document (TDD)

System Structure

The enhanced system is composed of several key components working in concert:
	•	Claude Code CLI / IDE Plugin: This is the entry point that the developer interacts with. It launches Claude Code sessions, sending user prompts to the Claude backend, and rendering Claude’s responses or diffs. In our design, the CLI is augmented to also handle setup and configuration tasks. For example, running claude-code init triggers a setup routine: generating files like .claudeignore and hook scripts, and configuring the MCP (Model Context Protocol) servers for memory. Technically, this might be implemented as a Python or Node.js script packaged with Claude Code, which on first run checks for the existence of required config (if not, it runs the init wizard). The CLI also reads .claudeignore to enforce those ignore rules at the tool level. This means when Claude tries to use its Read tool on an ignored path, the CLI intercepts and either denies it or strips that file from directory listings, ensuring secrets really stay secret. (This addresses the vulnerability noted where Claude could indirectly access ignored files via system messages ￼ – by having the CLI strictly filter file access).
	•	MCP Sub-Agent (Memory Server Connector): Claude Code uses the Model Context Protocol to communicate with external tools/servers. We will deploy the Qdrant MCP server (or a similar vector store agent) as part of our system. Technically, this is a process that runs (possibly started by the CLI on demand) and listens for tool requests from Claude. It exposes at least two operations: qdrant-store to save data to the vector DB, and qdrant-find to query data ￼. Our CLI’s init will configure Claude Code with the MCP server’s endpoint and provide descriptive instructions so Claude knows when to use it (for example, always search before coding) ￼. The MCP server itself wraps around the Qdrant vector database – if Qdrant is local, the server will start it or assume it’s running at localhost:6333. The data stored includes code embeddings and possibly discussion or plan embeddings (so Claude can recall not just code but also design decisions from earlier chat, if we choose to store those). Each repository might correspond to a separate Qdrant collection name, which the server can be instructed to use (the CLI can set environment variables like COLLECTION_NAME on startup) so that memory is segregated ￼. In summary, this component is the bridge between Claude and the vector memory – it responds to Claude’s semantic search queries with relevant code snippets and stores new code after generation for future retrieval ￼.
	•	Embedding & Indexing Engine: Behind the MCP server is the actual indexing logic. This could be implemented as part of the MCP server (for example, the Qdrant MCP server already handles incoming store requests by computing embeddings via a configured model). We will ensure it uses a lightweight embedding model (like all-MiniLM-L6-v2 or similar by default ￼) to balance performance and semantic accuracy. The indexing engine has two modes:
	1.	Bulk Indexer: A script or function that scans the repository (respecting .claudeignore patterns) and processes each file. It may chunk files if large, compute embeddings, and upsert them into Qdrant. This will be used on first-time setup or major re-sync events. We can implement this efficiently by using multiple threads or async IO to read files, and batch calls to the embedding model (if using an API or local model).
	2.	Incremental Indexer: A watcher that responds to changes. In the context of Claude Code, we might not have a long-running file system watch (to keep things simple), but we can hook into events that imply changes. For instance, after a Claude tool writes to a file, our after-write hook can immediately call the MCP qdrant-store for that file’s content. Similarly, on a git pull or branch change, we can have the CLI detect that event (possibly via the Stop hook or a git hook) and run an update for the changed files. We might utilize git diff between the previous and current HEADs to get a list of changed files and only re-index those, which is exactly the approach used for conditional tasks in hooks ￼. The incremental indexer ensures that by the time Claude might need context, the relevant pieces are in the DB. If it somehow misses an update, Claude can still fall back to reading the file directly, but maintaining the index means it often won’t have to, which saves tokens and time.
	•	Quality Rule Engine: This is central to the guard system. Technically, it could be implemented as a shell script or a Python script that runs at the end of each turn (Stop hook) and possibly in a lightweight form after each write. Let’s break its architecture:
	•	We maintain a set of rule definitions, possibly in a config file (e.g. .claude/rules/config.json) or simply as part of the script logic. Each rule can have metadata: name, trigger condition, check method, severity. For example, “Duplicate Code” rule might have trigger: “on Stop (full diff available)”, method: call a find_similar(code_snippet) against Qdrant for each new/changed function, severity: “block if similarity > 0.9 with existing code”. A simpler “No TODO comments” rule could have trigger: “on file write (check just that file)”, method: regex scan for ‘TODO’, severity: “warn (non-blocking)”.
	•	The Rule Engine script, when invoked, first determines context: if invoked at Stop, it might gather all changes (diff) in that turn. If at PostToolUse (after single file write), it deals with just that file. It then runs the applicable rules. This might mean sequentially going through rules that are enabled for that event. Efficiency is a concern: e.g., if 10 files changed, a duplicate code check might embed each new piece and search – we can optimize by batching searches if possible (maybe combine multiple queries in one Qdrant call, as it can handle vector queries in batches).
	•	For each rule violation found, the engine accumulates messages. If any violation is critical, it will cause the script to exit with code 2 after outputting the message. The message format is important: by convention, Claude Code surfaces stderr output back to Claude in a structured way ￼. We might format messages like “CRITICAL: Duplicate logic detected in utils.py (similar to helpers.py line 50).” or “WARNING: Possible insecure use of eval() in parser.js.” If exit code 2 is used, Claude will know it’s an error that needs fixing and the content of stderr will be fed into Claude’s next prompt (the system does this automatically). This mechanism is how we get Claude to actually address the issues.
	•	Some rules may not rely solely on static logic; for example, an “unsafe refactor” check might actually run tests or compare function outputs. For those, the rule engine can invoke external tools. Our design may incorporate a way to call npm test or other project-specific checks in the Stop hook as needed (similar to how one could wire Playwright tests in hooks ￼). If those external tests fail, that can be turned into a blocking error as well.
	•	Claude Sub-Agents for Analysis (Optional): For deeper analyses like complex refactor correctness or visual diffs, we can spin up sub-agents. Technically, Claude Code’s API might allow starting a sub-agent with a specific prompt and context, returning its result to the main session. For example, after Claude writes a large refactor, we could prompt a sub-agent: “Please double-check that the new implementation of function X behaves the same as the old one. Here is old code…, here is new code… Respond with ‘OK’ if identical or explain differences.” The main agent can then use that feedback (this sub-agent result could be parsed by our rule engine script). If the sub-agent says “There’s a change in edge-case behavior,” we flag a drift. This approach leverages AI for complex QA tasks beyond simple static rules. Similarly, a “visual critique” agent might be given CSS/HTML changes and asked to identify any potential UI/UX issues (like color contrast or missing translations). The architecture will allow plugging these in by defining them as tools or as part of the workflow when certain file types are present. Since sub-agents consume tokens and time, we’d invoke them only for high-impact scenarios (e.g., a major UI overhaul triggers the visual agent).
	•	Git Hooks Integration: In addition to Claude’s own hooks, we will install a standard Git pre-commit hook in the repo. This hook will call the same quality check script (or a subset of it) before allowing a commit. The reason is twofold: (1) If a user happens to commit changes outside of Claude Code (manually via git), we still want to catch issues. (2) It provides a safety net if Claude Code is not running at the moment of commit. The git hook can output any errors and abort the commit if a critical issue is found, instructing the developer to run Claude or fix manually. This ensures consistency: no bad code gets in even if someone bypasses the interactive session. The hooks installed will respect .claudeignore and likely use the memory DB as well (the hook script can query Qdrant via an API call or a lightweight client, meaning even outside Claude Code it can do semantic checks). This is an architectural decision to treat the quality guard as not just an AI feature but part of the repository’s tooling (much like a linter or test suite).
	•	Configuration Files: The system will use a combination of configuration files:
	•	.claudeignore: lists file patterns to exclude from AI access and indexing. Implemented similarly to .gitignore parsing. Ensures both memory indexer and Claude’s read access skip those.
	•	.claude/settings.json: extended to include our hook configurations (wiring PostToolUse and Stop to our scripts, as seen in examples ￼ ￼) and possibly memory settings (like the Qdrant server URL, collection name, etc., though those might also be environment variables).
	•	.claude/rules/ directory: could contain Markdown or JSON files defining custom rules or guidelines. This might be read by the rule engine. We can allow Markdown rules with patterns – e.g. a file no-secrets.md could list forbidden patterns. The rule engine can scan that directory and auto-generate checks (this gives teams a way to add simple rules without coding).
	•	CLAUDE.md (Project Memory): This is separate from our system, but we integrate with it. Architectural note: the presence of CLAUDE.md (shared memory file) and our Qdrant semantic memory should complement each other. We’ll ensure Claude gets both: CLAUDE.md for high-level guidance, and Qdrant for specific code context. They serve different scopes (one is explicit instruction, one is search-based memory).

The structure is modular: for instance, one could replace Qdrant with another vector DB by swapping the MCP server component, or add/remove rule modules without affecting the rest. Communication between components uses clear interfaces: Claude communicates with memory via the MCP protocol (HTTP requests triggered by find/store tool usage ￼), the hook scripts communicate with Claude via exit codes and stderr messages, and the CLI communicates with the filesystem and OS (setting up hooks, calling git commands, launching background services). This separation makes the system robust and easier to maintain or upgrade.

Indexing Pipeline

Initial Indexing (Bootstrap): When the user runs the setup command, the pipeline kicks off by scanning the repository. Pseudocode for this process:

files_to_index = all_files_not_ignored(repo_path, patterns_from_claudeignore)
for file in files_to_index:
    content = read_file(file)
    if content too large:
        chunks = split_content(content)
    else:
        chunks = [content]
    for chunk in chunks:
        vector = embed(chunk)
        qdrant_client.upsert(collection=project_id, vector, metadata={file_path, chunk_index})

We use .claudeignore to filter out files that shouldn’t be read or indexed (e.g. secrets, binaries). The embedding model might be a small Transformer model loaded locally for speed (if running locally) or a call to an embedding API. Using a local model like MiniLM means we can index offline and not send code externally, which some enterprise users will prefer. If performance is an issue, we can index in parallel using multiprocessing (embedding model inference can be parallelized if CPU-bound, or using multiple GPU threads if available).

We also create an ID or key for each vector that ties it back to the source location. This could be as simple as storing file path and a position range as metadata in Qdrant. That way, when we get a search hit, we know which file and where to fetch the snippet from, to present to Claude.

We incorporate intelligent caching: For example, maintain a local SQLite DB or even use Qdrant’s payload to store a hash of each chunk’s content. On subsequent re-index runs, compute the file’s current hash and compare to the last indexed hash; if unchanged, skip re-embedding. The initial index populates these hashes.

Incremental Updates (Git-Aware): Instead of watching every file system event (which can be heavy), we rely on git as the source of truth for changes:
	•	On Claude edits: We know exactly which file was changed (Claude’s Write tool invocation provides the path). After each such tool use, our PostToolUse hook can mark that file as “dirty for indexing”. We might not immediately embed it in the middle of a turn (to avoid slowing mid-turn actions), but we can queue it. However, since a Stop event will happen soon after a series of writes, it might be acceptable to just wait and do it at Stop along with other checks. Alternatively, a quick embed of one file after a write might not be noticeable (~100ms), so we could do it immediately to keep the index in sync.
	•	On Git operations: We can install a Git post-checkout and post-merge hook (via git config or our CLI) to notify our system of branch changes or merges. These hooks could simply write a file like .claude/index_pending or directly call a script if our environment is active. However, since Claude Code itself might execute the git checkout or git pull via its Shell tool, we can catch that in the Claude session: after the tool runs, Claude Code usually provides a system reminder about changed files. Our Stop hook can pick up on that and trigger re-index.
	•	For example, if the user says “checkout branch X” and Claude executes it, the Stop hook at end of that turn can detect a branch change (we can compare git rev-parse HEAD from before and after, stored in a temp file, or simply notice that multiple files changed).
	•	We then run a diff: git diff --name-status oldHEAD newHEAD to list added/modified/deleted files. We update the index accordingly: new files get embedded, modified files get re-embedded (unless the content hash is identical, indicating maybe just metadata changed), and deleted files get removed from Qdrant (Qdrant has a delete by payload or ID we can call).
	•	This process ensures that in a big merge, only those files changed will be processed, not the whole repo.
	•	On Periodic Full Index: We might implement a safeguard to do a full re-index once in a while or on user request. If the project is huge, re-indexing everything is expensive, so we lean on incremental. But if something goes wrong (e.g. Qdrant DB got wiped or we suspect it’s out-of-sync), the user can call e.g. /reindex memory and we’ll redo it. This is basically running the initial indexing procedure again. Because of caching (hash comparisons), this might actually be fast if most files are unchanged. We could even store the last indexed git commit hash; if the user tries to reindex and the head commit is the same as last time, we know nothing changed.

Hybrid Search Strategy: The indexing pipeline not only stores data, but also defines how we search:
	•	For quick similarity checks (like duplicate detection during quality check), we might not need full semantic embedding comparison; sometimes a simple text fingerprint could catch obvious duplicates faster. But to be robust (since code can be reformatted or variable names changed), we rely on semantic search for duplication as well.
	•	We use Qdrant’s approximate nearest neighbor search for efficiency. We can constrain the search space by using metadata filters. For instance, when searching for duplicates of a new function, we can filter to only search within the same programming language (metadata about file extension) or exclude the file itself. This speeds up search and improves relevancy.
	•	If needed, we can maintain a secondary index (like an inverted index of code identifiers) for specific rules. For example, detecting if a function name is already used elsewhere or if a certain logic (like a regex or SQL query) appears elsewhere might be done with a simple text search (which could be done via grep or ripgrep outside of Qdrant). Our pipeline could include building such secondary indices on the side, but to avoid complexity, likely we lean on Qdrant for everything unless profiling shows a clear need.

Resource Management: The indexing pipeline will be mindful of resources:
	•	Large files: We skip huge binary or data files entirely (and .claudeignore should cover them). Large code files are chunked to keep embedding queries reasonable.
	•	Memory: Qdrant can handle large amounts of vectors if on disk, but we ensure to set an appropriate segment size and maybe compression if needed. The embedding dimensionality (e.g. 384 for MiniLM) balances memory and recall accuracy.
	•	Concurrency: If multiple repos are active, each might have its own indexing thread or process. We must ensure they don’t compete too much. Possibly the CLI starts one indexing process per session that handles that session’s repo. They operate independently but could spike CPU if many run at once. It’s expected that the number of concurrent sessions is not huge (maybe 2-5 at most for a power user), so this is manageable.

Verification & Testing of Indexing: We will include checks to verify the index’s integrity. For example, after initial index, randomly pick a few functions and do a semantic search to ensure they find themselves (sanity check). Also, ensure that ignored content truly doesn’t appear in search results by querying some known secret string from .env and confirming nothing comes up. This gives confidence that .claudeignore is honored.

In summary, the indexing pipeline uses git-driven triggers to maintain an up-to-date semantic index without heavy continuous scanning, and uses caching and filtering to minimize redundant work. This hybrid strategy allows it to scale to large codebases while remaining responsive to changes.

Rule Engine

The Rule Engine is implemented as a combination of scripted checks and optional AI-based checks, orchestrated primarily during the Claude Code Stop hook (end-of-turn) and to a lesser extent during PostToolUse events.

Rule Definition and Configuration: We maintain a structured definition for each quality rule. This could be in code (a list of rules in the script) or in a config file for easy customization. Each rule has:
	•	Name: e.g. “Duplicate Code”, “Token Drift”, “Unsafe SQL Usage”.
	•	Scope Trigger: when to run it – e.g. “on any file change” vs “only at commit/stop” vs “only for certain file types or diff sizes”. For instance, we might mark “Token Drift” to run only on Stop when we have the final diff, whereas “No trailing whitespace” can run on every file save.
	•	Check Logic: either a function or external tool to execute. Some are simple (regex, lint tool), others are complex (semantic similarity search, running a test suite).
	•	Severity: classify as error (will block progress), warning (will notify but not block), or auto-fix (can be fixed silently without bothering the user).
	•	Auto-fix method (optional): if a rule violation can be fixed automatically, we can specify how. For example, “fix import ordering” can directly run a formatter; “remove debug print statements” can be done by editing the file via Claude’s Edit tool automatically.

We will likely implement the rule engine in a Python script (end_of_turn_check.py for instance), because Python allows easier integration with libraries (for static analysis or calling Qdrant HTTP API, etc.). This script will be invoked by the bash hook, e.g. .claude/hooks/end-of-turn-check.sh simply calls python3 .claude/rules/end_of_turn.py and handles its exit code.

Example: Duplicate Code Rule: When triggered, the script will take all new code chunks in this turn (it can get the diff from git – e.g. git diff --cached if Claude staged changes, or simply compare the last file versions saved). For each new function or block, it calls the memory search: e.g. qdrant-find with a query embedding of that block. It might retrieve top 3 similar code snippets. The script then analyzes the results: if the top result has very high cosine similarity (above a threshold) and comes from a different file (meaning this code likely already exists), we flag it. The message might be: “Possible duplicate of existing_func in moduleX.py (85% similarity). Consider reuse or refactor.” We mark this as an error if similarity is extremely high (essentially copy-paste), or maybe as a warning if it’s moderate similarity (it might be just similar due to doing a common task differently). If error, we exit 2 so Claude addresses it.

Example: Token Drift Rule: This one is a bit conceptual – it aims to catch when code that should stay in sync has diverged. Implementation approach:
	•	We could identify pairs of code that are meant to be aligned. This might be indicated by comments or memory notes (not trivial to auto-know). One way: if the project has tests or specifications, drift might mean the code doesn’t match the spec. Or if there’s duplicate logic that earlier was fine but now one changed and not the other.
	•	A heuristic: if duplicate code was allowed or existed historically, “drift” can be caught by noticing that one copy changed. For example, if function A and B were very similar, and now A is edited but B is not, we have drift. Our memory could store cross-references of similar code. So after each change, if we find a similar snippet in memory, we not only warn duplication but could also warn “You changed A but not B.” This is a complex case requiring historical knowledge – perhaps out-of-scope for initial version, but we design with it in mind.
	•	Another interpretation: “Token drift” could mean the AI’s plan drifted from the initial instructions (like it solved the problem in a different way than intended). We could implement a check comparing Claude’s final code to the spec or prompt. If, say, the user story said “don’t use recursion” but the code is recursive, that’s drift from requirements. This would require natural language or spec understanding, which is hard to generalize.
	•	Given ambiguity, we might initially implement “drift” detection focusing on ensuring consistency for known duplicates or template code. For example, if the project has a template file or duplicated code blocks with slight differences, the rule could highlight them for consolidation.

Example: Unsafe Structure Rule: This is essentially a catch-all for patterns considered dangerous or poor practice:
	•	We can incorporate an existing linter or static analysis tool (like ESLint for JS/TS, or a Python linter) configured with custom rules (for instance, flag use of eval, insecure crypto, etc.). Running such tools could be done either at Stop (for thorough analysis) or PostToolUse (if fast enough).
	•	Additionally, we can use simple pattern matching for things not covered by linters: e.g. “password” or “secret” literal in code (to avoid hardcoding secrets), large blocks of commented-out code left in, etc.
	•	If the project has a type of “structure” it wants to avoid (maybe global singletons or circular imports), those can be checked via a project-specific rule script.
	•	Each “unsafe” find would typically be an error to ensure it’s fixed. The guard might present a message like “Security issue: Detected use of MD5 for hashing, which is not allowed. Please use SHA-256 or better.” and block until resolved.

Fast Rules at Write-Time: We implement these as a separate script or part of the same script invoked with a flag. For PostToolUse (after each file write), we only run the rules that are very quick and file-local. This includes:
	•	Auto-formatting (already done via e.g. Prettier or Biome as in the example ￼).
	•	Basic lint (like no syntax errors, maybe using compilers or type checkers in watch mode – though compiling can be slow, so maybe skip).
	•	Simple regex rules (remove trailing whitespace, ensure newline at end of file, etc.).
	•	Possibly an incremental memory update (not a rule, but we might call the memory indexer here for the file).

These should all execute in under a second or two. The output at this stage is usually not shown unless something notable happens (for instance, after-write hook could just output “✓ Format applied” as a gentle notice).

Stop-Time Comprehensive Rules: At Stop, we run the superset of rules:
	•	We ensure that all quick rules also pass now (in case the user disabled auto-fix or something).
	•	Then run heavier stuff: semantic duplicate search, any test execution, cross-file checks, etc., as described.
	•	The Stop hook script will typically run in this order:
	1.	If the project requires it, install deps (like pnpm install) and run type-check (pnpm check) – as in the hooks guide example ￼. This ensures environmental consistency (especially if Claude added a library, we install it to run tests, etc.). We keep this step cached when possible (only install if new packages).
	2.	Run linters/tests: Could be configured by the user, but our design would support plugging this in easily. If tests fail, that’s an error (Claude should then try to fix the code to pass tests).
	3.	Run our custom rules (duplicate, drift, unsafe, etc.). These might run last because they inspect the final code state after any auto-fixes from steps 1-2.
	4.	Summarize any findings and decide to block or not.
	•	We use exit codes strategically: exit 0 means all good, exit 1 could mean warnings (Claude can continue, perhaps just noting them), exit 2 means block and require fix ￼. Claude Code specifically treats exit 2 as a signal to not proceed and instead let the assistant address the errors.

Integration with Claude’s Cycle: When our Stop script exits with code 2 and messages, what happens under the hood is Claude gets that stderr content injected into its conversation (likely as a system or assistant message that the last action failed and here’s why). Claude will then typically apologize (given its programming) and attempt to fix the issues. We rely on Claude’s own capabilities to interpret the error message. For example, if the error says “Type checks failed in file X line 20”, Claude will likely open that file and correct the type issue. This auto-remediation loop may repeat if new issues arise, but usually it converges quickly. Architecturally, this is a neat closed-loop: our rule engine finds issues and uses Claude itself to fix them, ensuring minimal user involvement.

Sub-Agent/AI Rule Assistance: Some rules can be enhanced by asking Claude (or a sub-agent) directly. For instance, an AI code reviewer sub-agent can be given the diff and asked “Are there any obvious issues with this change? Any potential bugs or violations of best practices?” The response might catch things our pattern-based rules missed (like a subtle logic bug). However, using the same Claude instance for this could be token-expensive and possibly redundant, since the main Claude could theoretically catch issues if prompted. But the main Claude isn’t explicitly prompted to critique its own work unless an error triggers it. So a sub-agent reviewer could act as an extra safety net. Implementation wise, this could be done by the Stop hook script making a call to the Claude API (outside of the main session) with a code review prompt, or if sub-agents are available internally, instruct Claude to spawn one. This is an advanced extension; in initial implementation, we rely on static rules plus any test suite outputs for judgments.

Performance Considerations: The rule engine must not introduce undue delay. We design it such that:
	•	The heavy semantic searches are minimized (only run for new/changed code, and possibly limited in number of queries). And Qdrant is pretty fast for queries (ms-level for few thousand vectors).
	•	Running tests or type checks can be the slowest part. If a project’s test suite is huge, running it on every turn might be impractical. The system encourages targeted test running (like only run relevant tests or use watch mode). In the example, they conditionally ran E2E tests only if .tsx files changed ￼ – we can do similar conditional logic for any expensive rule. We also might make some checks optional or configurable (e.g. run full test suite only on explicit user command or at final commit, not every turn).
	•	The script itself should be efficient (avoid O(n^2) scans or extremely inefficient regex on large files etc.). Use compiled regex, and process diff content rather than whole-file content where possible.
	•	If multiple rules look at similar data (e.g. duplicate and drift both use memory search), we can combine their queries to reduce overhead. For instance, do one vector search and use the results for both duplication and drift analysis.
	•	Multi-language support: The rule engine should detect file types and apply appropriate checks. It might use language-specific linters or rules only on those files. Unrecognized file types might be skipped or handled by generic rules.

Extensibility of Rules: To add a new rule, a developer should be able to either add an entry in a config or drop a script in a known directory. We might support both:
	•	Config-driven rules: e.g. a JSON entry that says “pattern: ‘console.log’, action: remove, severity: warning” to automatically strip console.log from code. The rule engine parser would incorporate that.
	•	Script/plugin rules: e.g. a script .claude/rules/custom_check.sh that outputs to stderr and uses exit codes. The main rule engine can call any executables in .claude/rules/exec/ or similar. For a Node.js project, a team might include a Node script to do domain-specific validation, which can be invoked from our Python engine as a subprocess.
	•	The architecture should make sure one misbehaving custom rule doesn’t break everything – maybe we sandbox their execution with timeouts. If a custom rule hangs or crashes, our engine can skip it and report an error to the user to check that rule’s implementation.

Finally, the Rule Knowledge Base: We will utilize memory for rules too. For example, if certain patterns have been fixed in the past, the semantic memory might catch them. Or we can store past bug fix diffs in Qdrant, so if a similar bug appears, Claude might be prompted by memory to recall that fix. This blurs into the AI’s own knowledge, but it’s a potential future extension. At the very least, our rules about style and best practices can be documented in the project’s CLAUDE.md or rules files so Claude is aware of them even before the hook catches something (e.g. CLAUDE.md: “We never use raw MD5 for hashing; always use Argon2”). This way, the system provides both preventive knowledge and reactive enforcement.

Hook System

Hooks are the glue that connect our quality guard and indexing to the Claude Code workflow. We leverage Claude Code’s event hooks (as documented in Anthropic’s guides ￼) and also Git hooks at the repository level.

Claude Code Hooks Setup: In the .claude/settings.json (installed by our init), we configure:
	•	PostToolUse event with a matcher for Write|Edit tools, pointing to our after-write script ￼. This ensures that right after Claude writes or edits a file, our script runs. The script (.claude/hooks/after-write.sh) will be intentionally minimal and fast ￼. Initially, it might just print a confirmation and potentially call a formatting tool. We might extend it to call a Python snippet that does a quick index update or checks trivial rules. The key is it must return quickly to not stall Claude’s flow. By design, PostToolUse hooks run before Claude gets to continue planning, so we want to get in, do the fix, and get out. If a formatting tool changes the file, Claude will actually get a system reminder that the file changed externally, which it can handle (it often re-reads the file if needed).
	•	Stop event with matcher *, pointing to our end-of-turn-check.sh ￼. This ensures at every assistant turn’s end (when Claude finishes its response), we run the comprehensive checks. This script will call our Rule Engine and possibly other tasks (like test runs). If it exits with code 2, Claude is blocked from continuing and instead given the error output ￼. If exit 0, the conversation proceeds normally (Claude might present the final diff to user or await user input).
	•	We could also use SessionStart if needed – for example, to ensure Qdrant is up and to perform an initial index diff. But since we do indexing at init and via git hooks, this might not be necessary. However, SessionStart could be useful to print a welcome message or to double-check memory connectivity (e.g., warn if Qdrant server is unreachable).

Git Hooks Setup: The system installs:
	•	.git/hooks/pre-commit: a script that runs claude-guard-precommit.sh (for example). This script can simply execute our Python rule engine in a “final check” mode, similar to Stop. If it returns non-zero, the commit is aborted with the messages shown. This ensures that even outside a Claude session, the same rules apply. One complexity: if a violation is found here, we don’t have Claude automatically fixing it (unless the user then invokes Claude). But at least it prevents bad code from entering version control. We will document to users that it’s best to use Claude for commits to auto-fix issues; manual commits might require them to heed the hook messages and fix things themselves or run Claude’s fix command.
	•	We might also set up a post-merge or post-checkout hook to trigger re-index. Alternatively, as mentioned, we handle that within the session. If users frequently switch branches outside of Claude, a post-checkout hook could call a small script to update the index (perhaps by launching a background Claude memory index update command). This is optional optimization.

Security Consideration: All our hook scripts are stored in the repository’s .claude directory, and we instruct the user to commit them (except perhaps the ones that should remain local, like anything with user-specific config). Running hooks means executing code on the user’s machine, so we have to ensure our scripts are safe (not exposing something malicious). Because our system is for the user’s own repo, this is usually fine (they can inspect the hooks). For teams, these hooks would be part of the repo configuration by maintainers.

Sequence of Hook Execution in a Turn: To illustrate:
	1.	Claude (the AI) decides to write some code. It uses the WriteFile tool.
	2.	Claude Code writes the file to disk. Immediately after that, the CLI triggers the PostToolUse hook. Our after-write.sh runs:
	•	It might run prettier --write file.js (for example) to format ￼.
	•	It prints “After-write: file changes applied.” ￼.
	•	It could also mark the file for indexing (maybe by calling a small Python snippet that notes the file path in an in-memory list to batch process later).
	•	It exits 0 (always, unless something truly unexpected happens).
Claude receives any stderr from this, which in our case is just an informational message. Claude typically ignores normal messages from after-write (or might incorporate “file changes applied” into its reasoning slightly, but it’s benign).
	3.	Claude continues its reasoning or finishes its task.
	4.	When Claude outputs the final answer for the turn (often showing a diff or a plan result), the Stop event fires. Our end-of-turn-check.sh executes:
	•	This script might first stage all changes (git add -A) in a temp index to make diffing easier or to run linters on the exact changes. (Alternatively, we can diff working tree vs HEAD to get changes).
	•	It calls end_of_turn.py (the rule engine). This runs through the heavy checks as described. Suppose it finds a critical duplicate code issue; it prints error to stderr and exits with code 2.
	•	The bash script sees exit 2, so it also exits 2.
	5.	Claude Code interprets the non-zero exit as something went wrong. Specifically, exit 2 is the convention for “blocking error” ￼. It takes the stderr content (“Duplicate code found…”) and feeds it into Claude’s conversation. The assistant is then prompted to respond to that.
	6.	Claude (the model) now sees a system message or similar about the error, and it enters a “correction loop”. The user sees Claude’s next message: “I see that the new code duplicates logic from moduleX. I will refactor to use moduleX instead.” Then Claude proceeds to make those changes (using Edit tool, etc.).
	7.	After Claude fixes the issue, the Stop hook will run again (end of that turn) to verify. If all clear, it exits 0 this time. Claude then concludes, possibly saying “Fixed the duplication issue. All checks passed.”
	8.	The conversation goes back to the user with code that has satisfied the guard.

Hook Matcher and Scope: We use matchers to ensure hooks run only when appropriate:
	•	The after-write hook is filtered to Write/Edit tools so it doesn’t run on every tool (e.g. not on Read or Shell outputs) ￼.
	•	The Stop hook we set to "*" matcher, meaning it runs for any Stop event regardless of what happened. We might refine this if needed (e.g., maybe don’t run if no file changes occurred?), but generally it’s fine to run each turn – the script can quickly exit if it detects nothing changed.
	•	Claude Code also has PreToolUse (before tools) and others. We could theoretically add a PreToolUse hook to prevent certain actions (like reading an ignored file – but we handle that via the ignore list itself). Another idea: a PreToolUse hook could prevent Claude from writing code if something else is not done (but that’s probably overkill and could hinder Claude’s normal operation, so we avoid unless a specific need arises).

Ensuring Hooks Don’t Conflict: The user might have their own hooks or other extensions. Our design tries to consolidate into a single Stop hook script that can call multiple things (linters, our rules, tests). So the repo isn’t cluttered with too many separate hooks. If multiple Stop hooks are needed (Claude supports a list), we can merge or chain them. E.g. if a user already had a Stop hook for something, our init might append our command to it rather than overwrite. We will document how to manually merge hook configs if needed.

Testing Hooks: We will test scenarios like:
	•	The hook correctly blocks and triggers Claude fix (we can simulate a known issue and see if Claude resolves it).
	•	Performance of hook: ensure that even with hooks the Claude session doesn’t time out. If hooks take too long, the AI might time out or the user gets impatient. We might set a reasonable upper limit (if a check runs too long, maybe print a warning and stop checks to let user know something took too long).
	•	Robustness: If the rule script crashes or the Qdrant is not reachable during a check (e.g., memory DB down), the hook should not hang indefinitely. We should catch exceptions in the Python script and perhaps exit 1 with a message “Memory check failed (server unreachable)” – likely not blocking, just warning, since it’s not a code issue but a tool issue. The user can decide to continue or fix the environment.

In summary, the hook system ties everything into Claude’s workflow, giving us deterministic control points to run our enhancements. It ensures that from the user’s perspective, the AI agent “magically” knows about these checks and memory retrievals without them having to invoke anything manually.

Performance Model

Balancing performance is critical. We analyze performance in terms of runtime overhead, memory usage, and token/compute efficiency:

Runtime Overhead:
	•	After-write Hooks: These are designed to be sub-second. Formatting a file or two is usually fast (especially if incremental). We will benchmark typical cases (e.g. Prettier on a 200-line file might take ~0.2s). If needed, we can optimize by formatting only changed lines or using in-memory formatting libraries. Linting at this stage will be superficial (anything more than a quick regex or so is deferred).
	•	Stop Hooks: This is where most overhead comes in. We strive to keep the Stop phase under ~5 seconds for moderate changes. For small changes, it should complete even faster (if no tests run and just a couple searches, maybe <1s). We rely on conditional logic to avoid unnecessary work:
	•	If no files were changed in a turn (e.g. maybe Claude only gave an answer without editing code), our script will detect that and skip running heavy checks.
	•	If only frontend files changed and we have a heavy backend test suite, we might skip backend tests, etc., based on change scope (this requires some project-specific config or conventions, but possible).
	•	Use concurrency where possible: For example, if we decide to do both a security scan and a duplicate search, do them in parallel threads and join – modern Python can utilize threads for I/O bound tasks (like calling an external API or reading files), though CPU-bound tasks (like computing many embeddings) might need multiprocessing due to GIL. We will identify bottlenecks and parallelize if it helps. The Qdrant search and possibly test running (if external) could be done concurrently.
	•	Claude’s own overhead: Our additions should ideally reduce overall token usage (due to memory hits), but could add some latency in terms of additional Claude turns when fixes are needed. However, these extra turns are the price for correctness. We assume users prefer a slight delay for an automated fix rather than having to fix later manually. We will monitor if the number of Claude turns per task increases significantly due to fixes; the aim is that often Claude gets things right by itself (with memory context) and no fix turn is needed, which is optimal.

Memory and Storage:
	•	The vector DB will consume memory proportional to the number of embeddings. Qdrant can handle millions of vectors, but on local dev machines, we might keep it to hundreds of thousands at most by indexing only code (no large text like big docs unless needed). For example, 1000 code files with average 5 chunks will be 5000 embeddings; at 384 dims float32, that’s ~7.7MB – trivial. Even 100k embeddings might be ~150MB, which is fine on modern disks. We can also enable quantization in Qdrant if needed to reduce memory.
	•	The Claude process context: By not stuffing all files into context and instead using retrieval, we hugely cut down the token context. Claude’s 100k token context is big, but reading entire large files is slow and expensive. With our memory, Claude will only get relevant snippets. This improves its performance and reduces cost.
	•	The overhead of running a local embedding model (if applicable): If using a CPU-based embedding for each code chunk, this can be heavy during initial index (but that’s one-time). During incremental, embedding one file’s content of, say, 100 lines is very fast (<0.1s on CPU typically). If needed, we can use a smaller embedding model or limit embedding frequency.

Parallel Repositories:
	•	If a user runs multiple sessions, e.g. two projects indexing concurrently, the system should handle it. Each session may run its own Qdrant or share one with separate collections. Qdrant is capable of handling concurrent requests, but heavy CPU tasks like embedding should be balanced. If the embedding model is loaded per session, we might have duplication in memory usage. A future optimization could be a centralized embedding service so that multiple sessions can send text to one service that queues them. However, initially, it’s acceptable that two parallel Claude sessions do their own embedding – such cases are likely rare and still small scale.
	•	Our performance model assumes typical usage: one primary active coding session at a time, with occasional parallel ones.

Tooling and Efficiency:
	•	Many of our checks reuse existing optimized tools (linters, compilers, etc.). Those tools (like TypeScript’s tsc, or Biome, etc.) are generally optimized in themselves (caching, incremental checks). We integrate with them in the intended way (for instance, running pnpm check which does incremental type checking ￼).
	•	The semantic search is optimized by Qdrant (which uses Approximate Nearest Neighbors). We might configure search parameters like EF and topK to balance speed vs accuracy. Even a topK of 5 is enough for our use-case (we don’t need 100 results).
	•	We also ensure that we don’t duplicate search calls. If Claude itself uses the qdrant-find tool to get context for writing code, we might reuse those results in our checks if possible. For example, if right before writing code, Claude did a search and used snippet Y (implying it found relevant code), our duplicate check might already suspect that reuse happened. In such case, maybe no error because Claude knowingly reused. But if Claude ignored a relevant snippet, that itself might be a drift – interestingly, if memory suggested something and Claude deviated, that could be flagged: memory said “function Y exists” but Claude wrote a new function Z anyway, which is duplication. If we can capture that scenario (maybe by logging all memory finds), we could flag it faster.

Profiling and Iteration:
We plan to instrument the hooks to log their execution time (maybe in a verbose mode). This helps identify slow steps. If certain projects find the Stop hook taking 15s consistently, we will see which step (embedding? test run? search?) is the culprit and target it. Our performance budget is that the AI’s own thinking (which can be several seconds per response) should dominate any overhead from our side. Ideally, the user perceives the overall process as slightly slower than vanilla Claude, but with far fewer back-and-forths to fix errors – so time-to-completion of a feature could even improve due to fewer iterations.

Scalability:
	•	For very large projects (say thousands of files), initial indexing cost is higher, but that’s one-time. In-session, Claude probably wouldn’t open thousands of files anyway due to context limits, so our memory approach actually scales better by not loading all that into context each turn.
	•	If an extremely large change happens (like a mass refactor across 100 files), the Stop hook will have to check a lot. In such a scenario, we might advise splitting the task or allow the guard to perhaps sample or limit its output (maybe treat as commit to run full suite). It’s an edge case, but the design doesn’t preclude handling it – just might be slower.

Network and External Calls:
	•	If Qdrant is local, queries are local – fast. If using a cloud Qdrant, network latency (say 50ms) is minor for our usage.
	•	If Claude Code itself is cloud-hosted (Claude Code Web), then our design changes a bit: the hooks and memory would run in a cloud environment provided by Anthropic. The conceptual model remains but some performance aspects then rely on their infra (we assume similar or they might have integrated memory differently).
	•	Our design is agnostic to that; we assume local development scenario primarily.

In sum, the performance model is tuned to make the guard as lightweight as possible in the common case, using caching, selective execution ￼, and fast algorithms, while still doing thorough analysis when it truly matters. We aim for the best of both worlds: high coverage of issues with minimal slowdown, achieving an “invisible” feel in normal operation.

Extensibility Guidelines

One of the design goals is to allow easy extension and customization of the memory and guard system. Here are guidelines for extending the system:
	•	Adding New Quality Rules: Developers (or we as maintainers) can introduce new rules without modifying core logic. This can be done by:
	•	Creating a new module in the rules directory. For example, to add a rule checking for offensive language in comments (just as an example), one can create .claude/rules/no_bad_words.py with a function that scans diff for blacklisted words and prints warnings. Our main rule engine will dynamically discover and run any .py or .sh in the rules directory if configured to do so. Alternatively, add a config entry in rules_config.json like:

{
  "name": "NoBadWords",
  "pattern": "(?i)\\b(badword1|badword2)\\b",
  "severity": "error",
  "message": "Inappropriate language detected"
}

If the engine sees a regex pattern rule, it will apply it across new lines in comments. The idea is to support non-programmers in adding simple rules via config, and programmers to add complex ones via scripts.

	•	Any new rule should specify how it can be auto-fixed (if possible) or if it only informs. For example, a “fix” function or a suggestion that Claude can implement. We encourage rule writers to consider if Claude can handle the fix automatically (in many cases it can).
	•	Testing new rules: We provide a way to test-run the rule engine (like a dry run mode) on a given diff or repository state, so developers can verify their custom rule behaves as expected before relying on it. Perhaps claude-code test-rules command to simulate.

	•	Pluggable Sub-Agents (Skills): The architecture should allow plugging in additional skills (sub-agents) that Claude can use. This might align with how tools are added via MCP. For example, if someone wants a “UIdiff-to-image” skill (that takes UI code and generates a screenshot), they could integrate an MCP tool for that. The guidelines:
	•	Define the skill’s interface (input/output). E.g., a tool ui-review that takes HTML/CSS and returns a report of UI issues.
	•	Register the tool with Claude via claude mcp add <tool> command (similar to how Qdrant is added) ￼. Provide a description so Claude knows when to invoke it. For instance, “Use this tool to critique visual design changes for usability and consistency.”
	•	Adjust the hooks if needed: maybe after Claude makes UI changes, we explicitly call the ui-review tool via a PostToolUse hook or as part of Stop checks. Alternatively, let Claude call it autonomously if it thinks it’s needed. But to ensure use, we might script it.
	•	Ensure the sub-agent’s results feed back into the rule engine: if the UI review returns something significant, treat it as a warning/error for Claude to handle.
	•	Because sub-agents can be very task-specific, we give them minimal context (maybe just relevant file diffs, not entire project) to keep them focused and efficient.
	•	Memory System Modularity: If a team wants to use a different vector DB (say Pinecone or an internal service) instead of Qdrant, they can. Our memory integration is through the MCP server abstraction. As long as there’s an MCP server that provides find and store semantics, Claude can use it ￼. We would document how to swap out Qdrant:
	•	Replace the claude mcp add qdrant-code-search step with another service’s config.
	•	Or if using Qdrant but want separate instances per project, that’s possible too (just the URL changes).
	•	Our system should not hardcode Qdrant specifics beyond the defaults. We could even detect if Qdrant is not installed and prompt to install or switch to an alternative.
	•	The .claudeignore and indexing logic should remain applicable with any backend (since it’s mostly client-side). Only the storage API changes.
	•	Upgrading and Maintaining Hooks: As Claude Code evolves, new events or features might become available (for instance, maybe a future version has a direct “QualityCheck” event). We plan for easy updates:
	•	The hooks configuration is centralized, so updating the hook scripts or adding a new event hook in settings can be done by updating the template and instructing users to re-run an init upgrade or do a minimal patch.
	•	We could version our config (maybe store a version number in settings.json) so we know if a project’s hook setup is outdated relative to the latest CLI.
	•	It’s important that if Claude Code itself implements some built-in quality guard in the future, our system can coexist or be toggled. We’d ensure our rules can be turned off or on to avoid double-checking the same thing. Extensibility includes being able to disable pieces easily. For example, a user might not care about one of our default rules and can disable it in config without editing code.
	•	Logging and Debugging: For extensibility and maintenance, robust logging is helpful. The system should log (perhaps to .claude/guard.log) the actions it takes: when it indexes, when it finds an issue, how long hooks took, etc. This is invaluable for developers extending the system or troubleshooting. Possibly include a debug mode environment variable to increase verbosity.
	•	If someone writes a custom rule and it’s not working, they could enable debug to see if their rule script was invoked or if it threw an error, etc.
	•	Community and Team Contributions: Encourage teams to codify their unique checks via this system. Perhaps maintain a repository of common rules plugins (like rules for certain frameworks, e.g. a rule that checks React setState usage or checks adherence to specific design patterns). Given the modular nature, sharing a rule is as simple as sharing a script or config snippet, which others can drop into their .claude/rules folder.
	•	Graceful Degradation: If for some reason a component is not present (say Qdrant is not installed or user chooses not to use semantic memory), the system should still function using Claude’s built-in capabilities (less effectively, but functionally). Our design therefore ensures that each extension is optional:
	•	If memory is off, Claude just won’t have that context, but it still can try with what it has.
	•	If the guard is off (user can toggle a “guard mode”), Claude will operate normally without enforced checks (maybe useful if the guard was causing issues and user is in a hurry).
	•	Perhaps provide a command /guard off or config to disable it, and similarly for memory. Extensibility includes the ability to turn features on/off easily.
	•	Testing Extensibility: We should test adding a dummy rule and see that it runs. Also test the scenario of multiple concurrent Claude sessions – ensure they don’t conflict by, say, writing to the same temp files or logs. Possibly name temp files with session or repo ID to avoid collision.

By following these guidelines, we ensure the system can evolve. The metaphorical architecture is that of a platform more than a fixed feature: it provides a framework where new “skills” (quality checks, analysis tools, memory sources) can be plugged in, much like how one can install browser extensions. This future-proofs the solution. Users can tailor Claude’s behavior heavily to their domain (e.g. game developers could add rules about performance anti-patterns, web developers could add accessibility checks, etc.), all while the core system remains stable and manages when/how these extensions run.

Ultimately, the extensibility aim is that the enhanced Claude Code feels like an open-ended, smart development assistant that teams can train and customize with their own knowledge. New rules and memory data effectively “teach” Claude about the codebase and standards, improving its output quality continually. The architecture supports this virtuous cycle with minimal friction, making the assistant’s capabilities feel like they naturally grow with the project – almost magical to the user, yet grounded in a solid, maintainable system design.
