"""Voyage AI embeddings implementation with retry logic and rate limiting."""

import time
from typing import Any, cast

from .base import EmbeddingResult, RetryableEmbedder, TiktokenMixin

try:
    import voyageai

    VOYAGE_AVAILABLE = True
except ImportError:
    VOYAGE_AVAILABLE = False


class VoyageEmbedder(TiktokenMixin, RetryableEmbedder):
    """Voyage AI embeddings with retry logic and rate limiting."""

    # Model configurations with current 2025 pricing
    MODELS = {
        "voyage-3": {
            "dimensions": 1024,
            "max_tokens": 32000,
            "cost_per_1k_tokens": 0.00006,  # $0.06/1M tokens = $0.00006/1K tokens
        },
        "voyage-3-lite": {
            "dimensions": 512,
            "max_tokens": 32000,
            "cost_per_1k_tokens": 0.00002,  # $0.02/1M tokens = $0.00002/1K tokens
        },
        "voyage-3.5-lite": {
            "dimensions": 512,  # Using 512d for smart upgrade (same storage as 3-lite)
            "max_tokens": 32000,
            "cost_per_1k_tokens": 0.00002,  # Same pricing as voyage-3-lite
        },
        "voyage-code-3": {
            "dimensions": 1024,
            "max_tokens": 32000,
            "cost_per_1k_tokens": 0.00006,  # Same as voyage-3
        },
    }

    def __init__(
        self,
        api_key: str,
        model: str = "voyage-3-lite",
        max_retries: int = 3,
        base_delay: float = 1.0,
    ):
        if not VOYAGE_AVAILABLE:
            raise ImportError(
                "VoyageAI package not available. Install with: pip install voyageai"
            )

        if not api_key or not api_key.strip():
            raise ValueError("Valid Voyage AI API key required")

        if model not in self.MODELS:
            raise ValueError(
                f"Unsupported model: {model}. Available: {list(self.MODELS.keys())}"
            )

        self.model = model
        self.model_config = self.MODELS[model]

        super().__init__(max_retries=max_retries, base_delay=base_delay)

        self.client = voyageai.Client(api_key=api_key)

        # Rate limiting - Voyage has different limits than OpenAI
        self._requests_per_minute = 300  # Conservative limit
        self._tokens_per_minute = 1000000  # 1M tokens per minute
        self._request_times: list[float] = []
        self._token_counts: list[tuple[float, int]] = []

    def _init_tiktoken(self) -> None:
        """Initialize tiktoken - Voyage uses similar tokenization to OpenAI's cl100k_base."""
        try:
            import tiktoken

            # Voyage tokenization is similar to OpenAI's cl100k_base
            self._tiktoken_encoder = tiktoken.get_encoding("cl100k_base")
            self.logger.debug(
                "Using cl100k_base encoder for Voyage (similar tokenization to OpenAI)"
            )
        except ImportError:
            self.logger.warning("tiktoken not available, using character approximation")
            self._tiktoken_encoder = None
        except Exception as e:
            self.logger.warning(f"tiktoken initialization failed: {e}")
            self._tiktoken_encoder = None

    def _check_rate_limits(self, estimated_tokens: int = 1000) -> None:
        """Check and enforce rate limits."""
        current_time = time.time()

        # Clean old entries (older than 1 minute)
        self._request_times = [t for t in self._request_times if current_time - t < 60]
        self._token_counts = [
            (t, tokens) for t, tokens in self._token_counts if current_time - t < 60
        ]

        # Check request rate limit
        if len(self._request_times) >= self._requests_per_minute:
            sleep_time = 60 - (current_time - self._request_times[0]) + 1
            if sleep_time > 0:
                print(f"Rate limit reached. Sleeping for {sleep_time:.1f} seconds...")
                time.sleep(sleep_time)

        # Check token rate limit
        total_tokens = (
            sum(tokens for _, tokens in self._token_counts) + estimated_tokens
        )
        if total_tokens >= self._tokens_per_minute:
            sleep_time = 60 - (current_time - self._token_counts[0][0]) + 1
            if sleep_time > 0:
                print(
                    f"Token rate limit reached. Sleeping for {sleep_time:.1f} seconds..."
                )
                time.sleep(sleep_time)

    def _estimate_tokens(self, text: str) -> int:
        """Accurate token estimation using tiktoken (cl100k_base)."""
        return self._estimate_tokens_with_tiktoken(text)

    def _calculate_cost(self, token_count: int) -> float:
        """Calculate estimated cost for token count."""
        cost_per_token = self.model_config["cost_per_1k_tokens"] / 1000
        return token_count * cost_per_token

    def embed_text(self, text: str) -> EmbeddingResult:
        """Generate embedding for a single text."""
        start_time = time.time()
        estimated_tokens = self._estimate_tokens(text)

        # Truncate if necessary
        text = self.truncate_text(text)

        def _embed() -> EmbeddingResult:
            self._check_rate_limits(estimated_tokens)

            response = self.client.embed(
                texts=[text],
                model=self.model,
                input_type="document",
                output_dimension=self.model_config["dimensions"],
            )

            # Record request for rate limiting
            current_time = time.time()
            self._request_times.append(current_time)

            # Voyage returns total_tokens in response
            actual_tokens = response.total_tokens
            self._token_counts.append((current_time, actual_tokens))

            return EmbeddingResult(
                text=text,
                embedding=cast(list[float], response.embeddings[0]),
                model=self.model,
                token_count=actual_tokens,
                processing_time=time.time() - start_time,
                cost_estimate=self._calculate_cost(actual_tokens),
            )

        try:
            result = self._embed_with_retry(_embed)
            return cast(EmbeddingResult, result)
        except Exception as e:
            return EmbeddingResult(
                text=text,
                embedding=[],
                model=self.model,
                processing_time=time.time() - start_time,
                error=str(e),
            )

    def embed_batch(
        self, texts: list[str], item_type: str = "general"
    ) -> list[EmbeddingResult]:
        """Generate embeddings for multiple texts.

        Args:
            texts: List of text strings to embed
            item_type: Type of items being embedded ('relation', 'entity', 'implementation', 'general')
        """
        if not texts:
            return []

        # Voyage token limits per API testing
        model_limits = {
            "voyage-3-lite": 30_000,  # Safe limit below 32K context (tested up to 32K)
            "voyage-3.5-lite": 30_000,  # Same as voyage-3-lite
            "voyage-3": 120_000,
            "voyage-code-3": 120_000,
        }
        token_limit = model_limits.get(self.model, 120_000)  # Conservative default

        # Optimize batch size based on content type
        # Relations are very short (~20-50 tokens), so we can batch many more
        if item_type == "relation":
            text_count_limit = (
                500  # Aggressive batching for relations (80% reduction in API calls)
            )
        else:
            text_count_limit = 100  # Standard batching for entities/implementations

        # Import progress bar if available
        try:
            from ..progress_bar import ModernProgressBar

            progress_bar = ModernProgressBar(
                total_items=len(texts), description="Generating embeddings"
            )
        except ImportError:
            progress_bar = None

        results: list[EmbeddingResult] = []
        current_batch: list[str] = []
        current_tokens = 0
        texts_processed = 0

        for text in texts:
            text_tokens = self._estimate_tokens(text)

            # Check both token limit AND text count limit
            if (
                current_tokens + text_tokens > token_limit
                or len(current_batch) >= text_count_limit
            ) and current_batch:
                # Process current batch
                batch_results = self._embed_batch(current_batch)
                results.extend(batch_results)

                # Update progress
                texts_processed += len(current_batch)
                if progress_bar:
                    progress_bar.update(texts_processed)

                current_batch = []
                current_tokens = 0

            current_batch.append(text)
            current_tokens += text_tokens

        # Process final batch
        if current_batch:
            batch_results = self._embed_batch(current_batch)
            results.extend(batch_results)

            # Update final progress
            texts_processed += len(current_batch)
            if progress_bar:
                progress_bar.update(texts_processed)

        # Complete progress bar
        if progress_bar:
            progress_bar.complete()

        return results

    def _embed_batch(self, texts: list[str]) -> list[EmbeddingResult]:
        """Embed a single batch of texts."""
        start_time = time.time()

        # Truncate texts if necessary
        truncated_texts = [self.truncate_text(text) for text in texts]
        estimated_tokens = sum(self._estimate_tokens(text) for text in truncated_texts)

        def _embed() -> list[EmbeddingResult]:
            self._check_rate_limits(estimated_tokens)

            response = self.client.embed(
                texts=truncated_texts,
                model=self.model,
                input_type="document",
                output_dimension=self.model_config["dimensions"],
            )

            # Record request for rate limiting
            current_time = time.time()
            self._request_times.append(current_time)

            # Voyage returns total_tokens in response
            actual_tokens = response.total_tokens
            self._token_counts.append((current_time, actual_tokens))

            # Create results for each text
            processing_time = time.time() - start_time
            total_cost = self._calculate_cost(actual_tokens)
            cost_per_text = total_cost / len(texts)
            tokens_per_text = actual_tokens // len(texts)

            results = []
            for _i, (text, embedding) in enumerate(
                zip(texts, response.embeddings, strict=False)
            ):
                results.append(
                    EmbeddingResult(
                        text=text,
                        embedding=cast(list[float], embedding),
                        model=self.model,
                        token_count=tokens_per_text,
                        processing_time=processing_time / len(texts),
                        cost_estimate=cost_per_text,
                    )
                )

            return results

        try:
            return cast(list[EmbeddingResult], self._embed_with_retry(_embed))
        except Exception as e:
            # Return error results for all texts
            error_msg = str(e)
            return [
                EmbeddingResult(
                    text=text,
                    embedding=[],
                    model=self.model,
                    processing_time=0.0,
                    error=error_msg,
                )
                for text in texts
            ]

    def get_model_info(self) -> dict[str, Any]:
        """Get information about the embedding model."""
        return {
            "provider": "voyage",
            "model": self.model,
            "dimensions": self.model_config["dimensions"],
            "max_tokens": self.model_config["max_tokens"],
            "cost_per_1k_tokens": self.model_config["cost_per_1k_tokens"],
            "supports_batch": True,
            "rate_limits": {
                "requests_per_minute": self._requests_per_minute,
                "tokens_per_minute": self._tokens_per_minute,
            },
        }

    def get_max_tokens(self) -> int:
        """Get maximum token limit for input text."""
        return int(self.model_config["max_tokens"])

    def dimension(self) -> int:
        """Get the dimension of embeddings for the current model."""
        return self.model_config["dimensions"]

    def get_usage_stats(self) -> dict[str, Any]:
        """Get usage statistics."""
        current_time = time.time()

        # Recent requests (last minute)
        recent_requests = [t for t in self._request_times if current_time - t < 60]
        recent_tokens = sum(
            tokens for t, tokens in self._token_counts if current_time - t < 60
        )

        # Total usage
        total_requests = len(self._request_times)
        total_tokens = sum(tokens for _, tokens in self._token_counts)
        total_cost = sum(
            self._calculate_cost(tokens) for _, tokens in self._token_counts
        )

        return {
            "total_requests": total_requests,
            "total_tokens": total_tokens,
            "total_cost_estimate": total_cost,
            "recent_requests_per_minute": len(recent_requests),
            "recent_tokens_per_minute": recent_tokens,
            "average_tokens_per_request": total_tokens / max(total_requests, 1),
        }
