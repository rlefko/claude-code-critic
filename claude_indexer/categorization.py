"""
File categorization system for differentiated processing.

This module provides intelligent classification of files into processing tiers
to optimize indexing performance without losing accuracy.
"""

import re
from enum import Enum
from pathlib import Path


class ProcessingTier(Enum):
    """Processing tier levels for files."""

    LIGHT = "light"  # Minimal processing - metadata only
    STANDARD = "standard"  # Normal processing - current behavior
    DEEP = "deep"  # Enhanced processing - critical files


class FileCategorizationSystem:
    """Categorizes files for optimized processing."""

    # Patterns for generated/auto-created files that need light processing
    GENERATED_PATTERNS = [
        # TypeScript type definitions
        r".*\.d\.ts$",
        r".*\.generated\.ts$",
        r".*\.generated\.tsx$",
        r".*\.generated\.js$",
        r".*\.generated\.jsx$",
        # Generated directories
        r".*/\_\_generated\_\_/.*",
        r".*/generated/.*",
        r".*/\.next/.*",
        r".*/dist/.*",
        r".*/build/.*",
        # API schemas and definitions
        r".*\.schema\.json$",
        r".*\.schema\.graphql$",
        r"openapi.*\.json$",
        r"swagger.*\.json$",
        r".*/api/schema/.*",
        r".*/types/generated/.*",
        # Lock files (though usually excluded)
        r".*-lock\.json$",
        r".*\.lock$",
        # Autogenerated test snapshots
        r".*\.snap$",
        r".*/__snapshots__/.*",
        # Database migrations (often auto-generated)
        r".*/migrations/\d{10,}.*",
        # Compiled/minified files
        r".*\.min\.js$",
        r".*\.min\.css$",
        r".*\.bundle\.js$",
    ]

    # Patterns for core/critical files that need deep processing
    CORE_PATTERNS = [
        # Entry points
        r".*/index\.(ts|tsx|js|jsx)$",
        r".*/main\.(ts|tsx|js|jsx|py)$",
        r".*/app\.(ts|tsx|js|jsx|py)$",
        r".*/__init__\.py$",
        # Configuration files
        r"^[^/]*config.*\.(ts|js|json|py)$",
        r"^[^/]*settings.*\.(ts|js|json|py)$",
        # Core business logic directories
        r".*/core/.*",
        r".*/domain/.*",
        r".*/business/.*",
        r".*/services/.*",
        r".*/handlers/.*",
        r".*/controllers/.*",
        # API routes and endpoints
        r".*/api/.*\.(ts|tsx|js|jsx|py)$",
        r".*/routes/.*\.(ts|tsx|js|jsx|py)$",
        r".*/endpoints/.*\.(ts|tsx|js|jsx|py)$",
        # State management
        r".*/store/.*",
        r".*/redux/.*",
        r".*/state/.*",
        r".*/context/.*",
    ]

    # File size thresholds for tier decisions (in bytes)
    LARGE_FILE_THRESHOLD = 500_000  # 500KB - large files get standard processing max
    HUGE_FILE_THRESHOLD = 2_000_000  # 2MB - huge files always get light processing

    def __init__(self):
        """Initialize the categorization system."""
        # Compile regex patterns for efficiency
        self.generated_patterns = [re.compile(p) for p in self.GENERATED_PATTERNS]
        self.core_patterns = [re.compile(p) for p in self.CORE_PATTERNS]

    def categorize_file(self, file_path: Path) -> ProcessingTier:
        """
        Categorize a file into a processing tier.

        Args:
            file_path: Path to the file to categorize

        Returns:
            ProcessingTier enum value
        """
        # Convert to string for pattern matching
        path_str = str(file_path)

        # Check file size first - huge files always get light processing
        try:
            file_size = file_path.stat().st_size
            if file_size > self.HUGE_FILE_THRESHOLD:
                return ProcessingTier.LIGHT
        except OSError:
            # If we can't stat the file, use standard processing
            pass

        # Check if it's a generated file (light processing)
        if self._is_generated_file(path_str):
            return ProcessingTier.LIGHT

        # Check if it's a core file (deep processing)
        # But only if it's not too large
        try:
            file_size = file_path.stat().st_size
            if file_size < self.LARGE_FILE_THRESHOLD and self._is_core_file(path_str):
                return ProcessingTier.DEEP
        except OSError:
            # If we can't stat, check patterns anyway
            if self._is_core_file(path_str):
                return ProcessingTier.DEEP

        # Default to standard processing
        return ProcessingTier.STANDARD

    def _is_generated_file(self, path_str: str) -> bool:
        """Check if file matches generated file patterns."""
        return any(pattern.search(path_str) for pattern in self.generated_patterns)

    def _is_core_file(self, path_str: str) -> bool:
        """Check if file matches core file patterns."""
        return any(pattern.search(path_str) for pattern in self.core_patterns)

    def get_tier_stats(self, file_paths: list[Path]) -> dict[str, int]:
        """
        Get statistics about file tier distribution.

        Args:
            file_paths: List of file paths to analyze

        Returns:
            Dictionary with counts per tier
        """
        stats = {
            ProcessingTier.LIGHT.value: 0,
            ProcessingTier.STANDARD.value: 0,
            ProcessingTier.DEEP.value: 0,
        }

        for file_path in file_paths:
            tier = self.categorize_file(file_path)
            stats[tier.value] += 1

        return stats

    def should_extract_relations(self, file_path: Path) -> bool:
        """
        Determine if relations should be extracted from this file.

        Light tier files skip relation extraction since they typically
        contain only type definitions and interfaces.

        Args:
            file_path: Path to the file

        Returns:
            True if relations should be extracted, False otherwise
        """
        tier = self.categorize_file(file_path)
        return tier != ProcessingTier.LIGHT

    def should_use_semantic_analysis(self, file_path: Path) -> bool:
        """
        Determine if deep semantic analysis should be used.

        Only deep tier files get expensive semantic analysis like
        Jedi for Python or advanced type inference.

        Args:
            file_path: Path to the file

        Returns:
            True if semantic analysis should be used, False otherwise
        """
        tier = self.categorize_file(file_path)
        return tier == ProcessingTier.DEEP

    def get_processing_config(self, file_path: Path) -> dict:
        """
        Get processing configuration for a file based on its tier.

        Args:
            file_path: Path to the file

        Returns:
            Dictionary with processing configuration
        """
        tier = self.categorize_file(file_path)

        if tier == ProcessingTier.LIGHT:
            return {
                "tier": tier.value,
                "extract_entities": True,
                "extract_relations": False,
                "extract_implementations": False,
                "use_semantic_analysis": False,
                "max_depth": 1,  # Shallow parsing
                "streaming": True,  # Use streaming if possible
            }
        elif tier == ProcessingTier.DEEP:
            return {
                "tier": tier.value,
                "extract_entities": True,
                "extract_relations": True,
                "extract_implementations": True,
                "use_semantic_analysis": True,
                "max_depth": -1,  # Full depth parsing
                "streaming": False,  # Full analysis needs complete AST
            }
        else:  # STANDARD
            return {
                "tier": tier.value,
                "extract_entities": True,
                "extract_relations": True,
                "extract_implementations": True,
                "use_semantic_analysis": False,
                "max_depth": -1,  # Full depth parsing
                "streaming": False,  # Standard processing
            }
